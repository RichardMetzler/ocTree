\chapter{Related work}
\label{sec:related_work}

% @FIXME: immer mal wieder drueberschauen
Since this work focuses on the impact that being immersed in an interactive virtual reality scene has on human attention when both focusing and performing tasks on 3D objects, this section will be subdivided into two parts.
First, publications discussing \textit{mesh saliency}, a commonly used term to describe regional perceived importance of digital representations of real-world (or synthetic) objects as well as 2D images, in the context of human perception will be presented and briefly described. The second section will provide a look into human behavior, based on cognition and outside stimuli in virtual reality environments. Both of these sets of scientific works will provide a solid knowledge of terms and methods commonly used in this field and describe the current state of the art.

\section{Mesh saliency and human perception}
\label{sec:mesh_saliency_and_human_perception}

Research on what human perception guides us to focus our attention on when presented a 3D representation of an object was begun just past the year 2000 and has been a continuous effort ever since. One commonly cited publication in this field is Lee \textit{et al.} \cite{lee2005mesh}. Based on low-level human visual attention \cite{koch1987shifts}, it introduces the term \textit{mesh saliency}, a measure for regional based importance of 3D meshes and also presents a way to compute it. This fully automatic process successfully predicts  what would be classified by most observers as prominent, visually interesting regions on a mesh, thus allowing mesh operations such as simplification \cite{cignoni1998comparison} and segmentation \cite{shamir2008survey} to produce results that are more appealing to the beholder.

The model for computing \textit{mesh saliency} is based on a center-surround comparison of local curvature. It is scale-dependent on a \textit{saliency factor} $\varepsilon$, which is based on the diagonal of the objects bounding box, and is able to identify salient features of a mesh, depending on their surrounding area. Geometrically complex regions, for example a large patch containing lots of bumps of similar size, will be rightfully dismissed as, in most cases, regions that are not interesting from a human perceptional stance.

Taking a closer look at the basic formula through which saliency for any vertex of a mesh can be computed according to Lee \textit{et al.} helps understanding the underlying concept. As a first step, the mean curvature map for a mesh, describing mean local curvature values on a point-level for each of its vertices, needs to be calculated via commonly known approaches such as \cite{taubin1995estimating}. The resulting mean curvature map $\mathscr{C}$ defines a mapping from each vertex of a mesh to its mean curvature $\mathscr{C}$(\textit{v}). Using a distance measure such as the Euclidean or geodesic method, one can compute the neighborhood \textit{N}(\textit{v},$\sigma$) of a vertex \textit{v} which then defines a set of points within a distance $\sigma$. The Euclidean appraoch was used in Lee \textit{et al.} and subsequently in the formula below.
Using these definitions, the authors denote the Gaussian-weighted average of the mean curvature by G($\mathscr{C}$(\textit{v}),$\sigma$) and present the following way of computing it.

\begin{align*}
G(\mathscr{C}(v),\sigma) &= \frac
	{
		\sum_{x \in N(v,2\sigma)}
			\mathscr{C}(x)exp	-||x-v||^2 \backslash(2\sigma^2)
	}{
		\sum_{x \in N(v,2\sigma)}
			exp -||x-v||^2 \backslash(2\sigma^2)
	}
\end{align*}

For computation of the Gaussian-weighted average, a cut-off factor for the filter is assumed at a distance of 2$\sigma$, in other words twice the distance that a vertex can have to another vertex to still be considered in its neighborhood.
Based on these definitions, the saliency $\mathscr{S}$(\textit{v}) of a vertex \textit{v} is defined as the abslute difference between the Gaussian-weighted averages, as seen in the formula below.

\begin{align*}
\mathscr{S}(\textit{v}) &= |\textit{G}(\mathscr{C}(\textit{v}),\sigma) - G(\mathscr{C}(\textit{v}),2\sigma)|
\end{align*}

In order to get more refined results, one can conduct multiple computations of \textit{mesh saliency} with different values for $\sigma$. Lee \textit{et al.} use the previously mentioned \textit{saliency factor} $\varepsilon$ with $\varepsilon \in 2,6$ in their paper, to generate multiple values for $\sigma$.

The concept of \textit{mesh saliency} has since been refined, augmented and adapted to serve as a basis for a multitude of specific use-cases and applications. When processing single vertex saliency, Wu \textit{et al.} \cite{wu2013mesh} took into consideration not only the curvature of the region surrounding the vertex, but also the global context of it. In other words, for each vertex to be attributed a value describing its saliency, its \textit{global rarity}, derived from comparing its features to those of every other vertex of the object, is computed. They performed a user study in which they had participants choose one out of two saliency maps for a set of objects, presented in a random order. One map was generated using their approach, the other one with the model presented in Lee \textit{et al.} Participants were asked to pick the one that was a closer representation of what they would have considered interesting regions and features. Since their method got picked in almost 58 per cent of cases, while the results produced by the model presented in Lee \textit{et al.} were favored in about 42 per cent, this can be considered a true improvement of the way \textit{mesh saliency} can be computed.

One approach to improve the method of finding salient elements in 2D images relied on paying extra attention to depth-information in \cite{ciptadi2013depth}. In this work, Ciptadi \textit{et al.} found that better results in terms of automatic identification of objects and surfaces could indeed be achieved this way. Transferring these insights into a 3D context is easy since visually complex models often base on multiple image-maps describing, among other information, depth values on the surfaces of the model.
In \cite{potapova2011learning}, the authors took a more task-driven approach to contribute to the concept of \textit{saliency}. Gathering color- and depth information about real-world scenes using a Kinect sensor, they extracted semantic cues about surface heights, relative surface orientations and occluded edges. Based on that data, they computed combined saliency maps which allowed them to assign real-world objects to four different categories, enhancing ways a robotic system can interact with them, providing the best possible points where the objects can be grasped and whether they are in reach at all or not (due to occlusion by other objects).

Another recent work aimed at identifying single, distinct elements and objects of 3D models was presented in \cite{koschan2003perception} by Koschan \textit{et al.} The authors propose a segmentation algorithm that utilizes a human perception phenomenon known as the \textit{minima rule} which suggests that contours of negative curvature minima can serve as boundaries of disjunct visual parts or elements. Another detailed comparison between automatically detected points of interests and what participants in a study actually declared as visually interesting points was drawn by Dutagaci \textit{et al.} in \cite{dutagaci2012evaluation}.

To verify the practical relevance of identifying salient regions and features on 3D meshes, Howlett \textit{et al.} \cite{howlett2005predicting} conducted a user study on whether it is possible to determine such features in advance. Based on observations gathered from eye-tracking device based user studies, they concluded that, especially with natural objects (animals, humans etc.), this was indeed the case. On top of that, they also reported a significant increase in visual fidelity on objects which were simplified based on saliency weight-maps, according to reports of study participants. Furthermore, in \cite{kho2003user} the authors used user-guided simplification to preserve higher levels of detail in areas of 3D objects that people deemed important to the recognizability of the object. After performing mesh simplification according to \cite{garland1997qslim}, enhanced by taking user-derived weight maps into account, the authors observed what they described as perceptually improved approximations of input objects.

In another highly noteworthy work by Munaretti \cite{munaretti2007perceptual}, the concept of \textit{mesh saliency} was extended to deformable, in other words animated, objects. The author presented a way to generate so-called \textit{multi-pose saliency}, a combination of multiple saliency maps compoted for static poses of a mesh. These static meshes can also be interpreted as keyframed poses for dynamic deformation, which makes this work a potentially outstanding contribution to any field where 3D objects are being animated.

The author found a remarkable improvement of the original way of computing \textit{mesh saliency} as presented by Lee \textit{et al.} by using geodesic distance \cite{surazhsky2005fast} instead of euclidean distance when comparing local curvature values and implemented a way to compute multiple saliency maps for different levels of detail.

\section{Human attention in Virtual Reality}
\label{sec:human_attention_in_virtual_reality}

While navigation in virtual reality space via a traditional desktop setup with input devices such as a mouse and a keyboard still seems to allow users to perform better in navigation tasks, they generally found navigation via a head-mounted display more natural and intuitive \cite{santos2009head}. It is worth noting that this work evaluated a series of user studies described in their respective papers which were published between 1997 and 2006, so it is safe to assume that recent VR technology would get much better results in comparison.
% Studien ausf√ºhrlicher beschreiben! (6/22 im originalen Paper)
Kwanguk \textit{et al.} \cite{kim2014effects} also found that fully immersive systems such as a CAVE setup evoked the highest sense of presence of users in a task performance situation.
\cite{pausch1997quantifying} has found that, while not being being able to help users perform search tasks in virtual space faster than with a desktop setup, users with a head-mounted display were able to complete the tasks with more certainty. They spent significantly less time re-examining areas, which they commonly did with the desktop setup, sometimes multiple times.

In 1998, Atchley \textit{et al.} \cite{atchleyattentional} conducted four experiments addressing attention in 3D scenes. Participants were shown simple scenes, each containing sets of six short lines. The scenes were arranged on four different depth planes, one behind the other, and displayed on a stereographic display. The basic task given to participants in all of the experiments was to focus a briefly visible color singleton on a specific, previously cued depth plane. One of the lines on the indicated plane would change its color for 100 milliseconds and participants had to correctly say whether it was tilted to the left or the right. To determine the time it takes to shift attention from one plane to another, for some user groups, the color singleton would appear in a plane other than the previously hinted one. To further track speed and accuracy of attention focus, a distraction element (one additional line changing its color simultaneously to the target line) was shown to some participants, sometimes in the \textit{target plane}, sometimes in a different one. From their observations, the authors gathered that depth-plane attention can be successfully guided and that distraction elements appearing on the \textit{target plane} significantly interfered with the users' ability to give correct answers, while such elements appearing on other planes had virtually no impact on results.

Taking the effort of tracking attention one step further into virtual reality context, Lee \textit{et al.} \cite{lee2007real} accomplished just that on an object basis. They presented a framework capable of such a task, both bottom-up (stimulus-driven) and top-down (goal-directed). Based on pixel-level saliency maps, computed via known methods, similar maps for multiple objects in the scene are generated, allowing predictions on which objects will more likely to be focused first by users. Using the method presented in this paper, object-level saliency maps can be computed in real-time, depending on the users dynamic position and orientation within the scene. The authors exploited knowledge of human cognition which suggests that attention is object-based \cite{o1999fmri} and, using a monocular eye tracking device, compared the results of estimated object-level saliency maps to the behavior of participants in their study. They dynamically assigned saliency values to each object and, depending on how many of the objects with the highest values (first 1, 2 or 3) were taken into consideration, observed estimation accuracies ranging from about 50 per cent to up to nearly 95 per cent. As one can imagine, accuracy values were the highest when users were given a task, for example finding a certain object within the scene. This shows that attention in virtual space can be tracked and accurately predicted on object level.

In \cite{sokolov2008virtual}, the authors Sokolov \textit{et al.} took a more general approach to navigation in virtual 3D scenes, aimed at automatic generation of a set of viewports that allow the observer to gain maximum knowledge of the scene. This work provides detailed information on how viewport quality can be determined based upon the relation of the total numbers of visible, occluded and degenerated faces.
Sandor \textit{et al.} took the concept of saliency to a augmented reality context in \cite{sandor2010augmented} and presented the second iteration of their method of combining a real-time camera feed with a 3D overlay. Based upon clues from human perception, for each frame, the authors identified visually important objects and prevented them from being occluded or obscured by the 3D scene projected into it.
