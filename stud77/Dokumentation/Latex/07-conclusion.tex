\chapter{Conclusion and future work}
\label{sec:conclusion_and_future_work}
This section is a contains a brief summery of chapter \ref{sec:results_and_discussion} as well as an outlook on what can be done from now on. During this work, lots of data was collected but evaluation merely began on a surface level, due to other major tasks taking up so much of the workload.

\section{Insights and conclusions}
\label{insights_and_conclusions}

Difference ratios expressed via the measure of difference as suggested in section \ref{sec:measure_of_difference} are fairly low, ranging from about 0.23 for model one, to 0.27 for model two, up to 0.38 for model three. However, the expresiveness of this ratio is dubious. From a basic, visual comparison of computed \textit{mesh saliency} and \textit{user saliency} maps for model three, one might conclude that the two results are almost nothing alike. Based on processing via \textit{mesh saliency}, almost every vertex of the mode seems to have a near-zero importance ranking. The exception, the only parts that stand out in the \textit{mesh saliency map} for this very mechanical object, are fairly small geometrical details, exhaust pipes in this specific case. This is evident even when mapping non-normalised \textit{mesh saliency} values to the object, as shown in figure \ref{fig:results_ms_mechanical}. Again, it not plausible that this single number, this measure of distance, can be the basis of any sort of statement on how much average user selection actually differs from what can be regarded \textit{important} through computational processing of the 3D data. It is to be seen as a suggestion, a possible starting point for more sophisticated ways of quantifying differences in saliency maps.

However, based on trivial observations of the resulting \textit{importance} and difference maps, the measure of difference suggested in this work might still be useful for a rough estimation of which objects are more likely to have greater differences between \textit{mesh} and \textit{user saliency} value mappings. Depending on how strongly ratios computed via this method differ from one another, perhaps even a ranking of objects in terms of which of them are most likely to show the most / the least overall differences in saliency values, in relation to one another.

\textit{User saliency} and \textit{mesh saliency} maps show visual similarities for models one and two, as one might have expected. Especially when it comes to \textit{finding} contours, the results were close, even though the visual representation of computed \textit{mesh saliency} maps was vastly different than to that of \textit{user saliency} and difference maps. For all three objects, users seemed to put a heavier emphasis on parts that might possibly be described as \textit{characteristic} to them. These parts may times contradict the result of processing the object via \textit{mesh saliency}. This seems like a likely outcome of the user study, as users were encouraged to select parts of the objects according to their own, personal interpretation of the term \textit{interesting}. Human cognition and the process of identifying objects as based on recognising larger shapes rather than point-wise curvature. This might, in part, explain the emphasis on \textit{characteristic} parts but diving into human neurology any more would go beyond the scope of this work. This observation is to be interpreted merely as one of many possible aspects to understanding what users - especially in an immersive VR environment - find \textit{interesting} when presented 3D models.

One of the main problems with the measure of difference is that all vertex-wise saliency values are normalised, i.e. divided by their respective maximum values, before getting subtracted from one another. This was done with the goal in mind of getting one percentage ratio which can describe the overall difference of importance distributions but expresiveness, especially with \textit{mesh saliency} values gets lost this way. This is caused by maximum \textit{mesh saliency} values being very sprase and, seemingly, not surrounded by other values that also have high computed saliency values.

The result of normalising \textit{saliency} values, in combination with the fact that processing via \textit{mesh saliency} seems to produce low values for the vast majority of overall vertices, is that the amount of almost zero-saliency values is high for each object. On the other hand, there are many parts of objects which are also of no interest to users in VR, which leads to a great amount of \textit{user saliency} being zero as well. As a consequence, a great number of raw differences also amount to almost zero. So, put bluntly, for each high difference value, there might be so many near-zero values that the one high difference gets completely \textit{watered} and ultimatively loses its meaning through division by the number of vertices total.

Evaluation of model three (P51), being a mechanical object, insightful in another regard. It further impairs plausibility of the measure of difference, as both its \textit{saliency maps} look almost nothing alike. It also might implay that the original, unaltered version of \textit{mesh saliency} as described in \cite{lee2005mesh} is not suitable for mechanic objects. However, even contemplating genereal rules as to what shapes and which levels of detail are likely in mechanic objects in general, is a challenge unfathomable in its scope and complexity. So expecting any automated computational model to reliably predict regions of high interest in such objects is absurd.

Models one and two showed a much stronger variation in saliency values as well as sometimes very clearly distinct patches of \textit{essential importance}. Based on the data gathered during the user study, it is safe to assume that virtually every participant selected, for example, the region around the eyes of model two (Cow) as well as large portions of the ears of model one (Bunny). These highly interesting regions seemingly having such distinct borders might be a resourceful contribution to efforts of making the model \textit{mesh saliency} more coherent with what the average human beholder of 3D data finds \textit{interesting}. The effects of selection happening in an immersive virtual reality environment is to be further examined as well, since the feeling of standing in front an actual, live animal might greatly influence how people perceive it. This obviously begs the question whether immersion in VR can be measured, at which level such effects become noticable and how strong they scale with the quality of VR soft- and hardware.

\section{Future work}
\label{future_work}

This work is a first step to examining differences between what human users and mathematical procedures identify as \textit{interesting} parts of 3D data. Collecting data, suggesting a set of basic requirements to software that allows such data to be gathered, and a quick estimation as to whether there are significant differences or not is the main ambition of it. As a result, there is plenty of promising follow-up work. This section describes three possible branches of expanding further into the higher level questions at hand, motivating this work.

	\subsection{Improve measure of difference}
	\label{sec:improve_mod}
As discussed multiple times in this work, the measure of difference suggested in section \ref{sec:measure_of_difference} has lots of potential ways to improve upon it. As it is described in this work, it might still be useful as a ranking of which objects are likely to have greater overall differences of saliency values than others but the following aspects can and should be developed further.

\begin{itemize}
	\item Normalising both \textit{mesh} and \textit{user saliency} values (dividing them by the respective maximum value) causes results to get \textit{watered} by an abundance of near-zero values. Finding away around this, keeping the expressiveness of importance values, is an important first step towards an improved measure of difference.
	\item The idea of boiling down overall differences of 3D data to one single percentage ratio is meant to provide an estimation of differences as quickly and easily understandble as possible. However, a way of defining multiple difference ratios is certainly needed. This task can be tackled in multiple ways, obviously. Subdividing the whole range of computed values to discrete steps and computing difference values for each range is one of many trival suggestions as to how this can be achieved.
	\item Pattern recognition techniques can provide a massive boost to expresiveness of any measure of difference in this context. If contiguous patches of vertices with similar saliency values can be identified, difference ratios for each such patch can be computed. Whether the same, and how many, such patches even get identified through user input would be the next, trival step.
\end{itemize}

	\subsection{Variables}
	\label{sec:variables}
A lot of variables during the user study conducted for this work were set and not altered with the intention of getting similar information about similar data that is easy to compare. However, many of them can altered to achieve further insights.

\begin{itemize}
	\item The selection radius of the \textit{selection application} is computed based on the structure of the object. To be precise, a combination of such structure and the essential parameters passed to the octree structure which indexes the loaded 3D information, is the basis for said radius. Again, it is 95\% the size of the smallest possible subtree node, determined by \texttt{maxSplitDepth}. For obvious reasons, this radius is of essential importance to how the selection process feels. Trying other combinations of determining this radius, maybe even allowing users to dynamically alter it during selecting parts of an object, is not only a desirable functionality of the application but is also certain to provide more insightful and rich data.
	\item The size, shape and color of the selection target has virtually unlimited possibilites. Whether it is shown at all, how it is shown and its behavior when passing through the projected surface of a model are promising aspects to try and use other behaviors for.
	\item The \textit{selection application's} visual design is described in \ref{sec:conduct_user_study_with_the_selection_application}. Objects can be shaded differently, textures can be used, dynamic light effects can be used and much more can be done to alter the users immersion and experience within the scene.
	\item The upper time limit for users to finish their selection process on a single object was five minutes. This was chosen to prevent the user study from being too long and frustrating the user. However, it might be set to a different value, perhaps based on complexity of the object of hand.
\end{itemize}

	\subsection{More extensive user study}
	\label{sec:more_extensive_user_study}
This is again trivial. 32 users took part in the user study conducted for this work, a lot of its conditions were set from the beginning and remained unchanged for its entire duration. It can be greatly expanded in one or more of the following ways.

\begin{itemize}
	\item Get more users to acquire a richer pool of data. Best case, multiple hundreds of user inputs are certain to generate more refined, more generally applicable results.
	\item Other types of immersive VR might get tested and results can be compared with regards to the type of input hardware they were created with. In this work, the user study took place in a multi-wall projection installation. The first thing that comes to mind as an alternative might be a head-mounted display VR setup.
	\item Collecting more types of data is certain to allow for more thorough conclusions to be drawn from gathered data. Getting users to leave more qualitative, descriptive feedback comes to mind but there are lots of other ways to augment research data like this.
\end{itemize}

	\subsection{Unused data}
	\label{sec:unused_data}
In addition to vertex IDs in user selections - the central information needed for the comparison of overall differences as desigend in this work - a lot of other data was collected during the user study. Some of it is only mentioned in this work, other is not yet discussed due to necessary limitations to the scope of this work. The following types fo information are available.

\begin{itemize}
	\item As briefly stated in section \ref{sec:unweighted_difference_ratio}, user selection logfiles contain timestamps (in seconds since the application was started) next to each logged vertex ID. This timestamp expresses when the respective vertex was last added to or removed from the selection. While information on which vertices were removed is an interesting piece of information in itself, this timestamp might be used to determine which parts of the object were selected first and what type of implications on their perceived importance this might have.
	\item Demographic data about population of the user study as well as interest in technology are anonymously saved for each participant. It is easy to assume things like personal interest in technology, profession and maybe age contribute massively to how satisfactory using the \textit{selection application} is for the user. Further examining this is another possible piece of follow-up work to this thesis.
\end{itemize}

